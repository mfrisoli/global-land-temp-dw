{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### WORLD CITIES LAND TEMPERATURE ETL & DATA WAREHOUSE\n",
    "\n",
    "#### Project Summary\n",
    "ETL Pipeline to Data Warehouse in AWS Redshift \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from helpers.get_kaggle_data import kaggle_download\n",
    "from helpers.upload_to_s3 import upload_file\n",
    "from helpers.data_quality_checks import data_quality_check\n",
    "from helpers.sql import sql_create_tables, staging_insert, sql_drop_tables, sql_insert_tables\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "A Data Warehouse will be created to store clean organize data that will be accesable to multiple users on a AWS Redshift cluster. \n",
    "\n",
    "The ETL Pipeline will work as described below:\n",
    "\n",
    "- A Kaggle API will be used to get and download the data\n",
    "- Pandas will be used to explore and clean the data\n",
    "- Dataframe will be saved as CSV file and uploaded to AWS s3 bucket\n",
    "- Data will be imported to AWS Redshift\n",
    "- Data quality tests will be carried out to make sure the data exists and is available \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data from Kaggle API\n",
    "# You must have an account with Kaggle\n",
    "kaggle_download('berkeleyearth/climate-change-earth-surface-temperature-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "directory = os.path.realpath(\"..\") + '/global-land-temp-dw/'\n",
    "tables = ['GlobalLandTemperaturesByMajorCity.csv', 'GlobalLandTemperaturesByCity.csv']\n",
    "\n",
    "df_major_city = pd.read_csv(directory + \"data/\" + tables[0])\n",
    "df_city = pd.read_csv(directory + \"data/\" + tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DF for info major cities\")\n",
    "df_major_city.info()\n",
    "\n",
    "print(\"\\nDF info for cities\")\n",
    "df_city.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null/Missing Values\n",
    "df_city.isnull().sum()\n",
    "\n",
    "print(\"Null/Missing Values for major cities\")\n",
    "print(df_major_city.isnull().sum())\n",
    "\n",
    "print(\"\\nNull/Missing Values for cities\")\n",
    "print(df_city.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA/missing values This data requires all values to be available\n",
    "def drop_na(pd_df):\n",
    "\n",
    "    print(f'Number of rows before removing NA: {pd_df.shape[0]:,}')\n",
    "\n",
    "    pd_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    print(f'Number of rows after removing NA: {pd_df.shape[0]:,}')\n",
    "\n",
    "\n",
    "drop_na(df_major_city)\n",
    "drop_na(df_city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for Boolean Major city\n",
    "df_major_city = df_major_city.assign(major_city='true')\n",
    "df_city = df_city.assign(major_city='false')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both datasets\n",
    "dframes = [df_major_city, df_city]\n",
    "df = pd.concat(dframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by year and reset index\n",
    "df.sort_values(by='dt',inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm Data types are as expected\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV to load into SQL redshift\n",
    "df.to_csv(directory + \"data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Bucket Name from configurations \n",
    "config = ConfigParser()\n",
    "config.read('DW.cfg')\n",
    "file_log = config.get('S3', 'log_data')\n",
    "\n",
    "upload_file(directory + \"data/dataset.csv\", file_log, \"logs/log_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "source": [
    "### Star Schema"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "source": [
    "# Get Database Variables\n",
    "config = ConfigParser()\n",
    "config.read('DW.cfg')\n",
    "\n",
    "print(\"{}, {}, {}, {}, {}\".format(*config['CLUSTER'].values()))\n",
    "\n",
    "\n",
    "# Connect to redshift database\n",
    "conn = psycopg2.connect(\"\"\"host={} \n",
    "                           dbname={} \n",
    "                           user={} \n",
    "                           password={}\n",
    "                           port={}\"\"\"\\\n",
    "                           .format(*config['CLUSTER'].values()    \n",
    "))\n",
    "\n",
    "# conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all table if exists\n",
    "for query in sql_drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "for query in sql_create_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to Staging\n",
    "s3_bucket = \"s3://{}/logs/\".format(config.get('S3','log_data'))\n",
    "\n",
    "sql_query = staging_insert.format(\n",
    "    s3_bucket,\n",
    "    config.get('IAM_ROLE','ARN'),\n",
    "    config.get('CLUSTER','dwh_region')\n",
    "    )\n",
    "# print(sql_query)\n",
    "cur.execute(sql_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data from staging to facts and dimensions\n",
    "for query in sql_insert_tables:\n",
    "    # print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['readings_by_city', 'cities', 'time']\n",
    "\n",
    "for table in tables:\n",
    "\n",
    "    data_quality_check(cur, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = [\n",
    "    {   \n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'city_id',\n",
    "        'data type': 'BIGINT IDENTITY(0,1)',\n",
    "        'data format': 'NNNNNNN',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'Primary Key Index number for each row',\n",
    "        'example':'03'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'city',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': '',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'city name, comes from staging_events',\n",
    "        'example':'Barcelona'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'country',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': '',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'country name, comes from staging_events',\n",
    "        'example':'Venezuela'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'latitude',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': 'NN.NT',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'city latitude, comes from staging_events',\n",
    "        'example':'12.5N'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'longitude',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': 'NN.NT',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'city longitude, comes from staging_events',\n",
    "        'example':'12.5E'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'major_city',\n",
    "        'data type': 'BOOLEAN',\n",
    "        'data format': 'true/false',\n",
    "        'field size': '5',\n",
    "        'description':'Boolean that identifies if city is major or not from staging_events',\n",
    "        'example':'true'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'dt',\n",
    "        'data type': 'DATE',\n",
    "        'data format': 'YYYY-MM-DD',\n",
    "        'field size': '10',\n",
    "        'description':'date of temperature measure, comes from staging_events',\n",
    "        'example':'2018-02-07'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'day',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'day of temperature measure, comes from column dt',\n",
    "        'example':'07'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'month',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'month of temperature measure, comes from column dt',\n",
    "        'example':'02'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'week',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'week of temperature measure, comes from column dt',\n",
    "        'example':'02'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'weekday',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'week day number of temperature measure, comes from column dt',\n",
    "        'example':'02'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'year',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NNNN',\n",
    "        'field size': '4',\n",
    "        'description':'year of temperature measure, comes from column dt',\n",
    "        'example':'2018'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'by_city_id',\n",
    "        'data type': 'BIGINT IDENTITY(0,1)',\n",
    "        'data format': 'NNNNNN',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'Primary Key Index number for each row',\n",
    "        'example':'2'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'city_id',\n",
    "        'data type': 'NUMERIC',\n",
    "        'data format': 'NNNNNN',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'Foreign Key Index number from cities table',\n",
    "        'example':'4525'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'avg_temp',\n",
    "        'data type': 'NUMERIC(7,3)',\n",
    "        'data format': 'NNNN.NNN',\n",
    "        'field size': '7',\n",
    "        'description':'temperature reading comes from staging_events',\n",
    "        'example':'25.025'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'avg_temp_uncertainty',\n",
    "        'data type': 'NUMERIC(7,3)',\n",
    "        'data format': 'NNNN.NNN',\n",
    "        'field size': '7',\n",
    "        'description':'temperature uncertainty reading comes from staging_events',\n",
    "        'example':'1.025'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'date',\n",
    "        'data type': 'DATE',\n",
    "        'data format': 'YYYY-MM-DD',\n",
    "        'field size': '7',\n",
    "        'description':'date of temperature measure, comes from staging_events',\n",
    "        'example':'2018-02-07'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'major_city',\n",
    "        'data type': 'BOLEAN',\n",
    "        'data format': '',\n",
    "        'field size': '5',\n",
    "        'description':'Boolean that identifies if city is major or not from staging_events',\n",
    "        'example':'true'\n",
    "    },\n",
    "    \n",
    "    \n",
    "       \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic_df = pd.DataFrame(data_dictionary)\n",
    "\n",
    "data_dic_df.to_csv(directory + \"media/data_dict.csv\")\n",
    "\n",
    "data_dic_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('land-temp-dw': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6928e04d17b1ec80b230eec5c511cba50c562c46b302dc929486dd2dac5f22f5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}