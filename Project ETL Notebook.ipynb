{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### WORLD CITIES LAND TEMPERATURE ETL & DATA WAREHOUSE\n",
    "\n",
    "#### Project Summary\n",
    "ETL Pipeline to Data Warehouse in AWS Redshift \n",
    "\n",
    "The project follows below steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from helpers.get_kaggle_data import kaggle_download\n",
    "from helpers.upload_to_s3 import upload_file\n",
    "from helpers.data_quality_checks import data_quality_check, data_integrity_check\n",
    "from helpers.sql import sql_create_tables, staging_insert, sql_drop_tables, sql_insert_tables\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "A Data Warehouse will be created to store clean organize data that will be accesable to multiple users on a AWS Redshift cluster. \n",
    "\n",
    "The ETL Pipeline will work as described below:\n",
    "\n",
    "- A Kaggle API will be used to get and download the data\n",
    "- Pandas will be used to explore and clean the data\n",
    "- Dataframe will be saved as CSV file and uploaded to AWS s3 bucket\n",
    "- Data will be imported to AWS Redshift\n",
    "- Data quality tests will be carried out to make sure the data exists and is available \n",
    "\n",
    "#### The Data:\n",
    "\n",
    "The source of the data used for this project comes from Kaggle [Climate Change: Earth Surface Temperature Data Set](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "\"Some say climate change is the biggest threat of our age while others say it’s a myth based on dodgy science. We are turning some of the data over to you so you can form your own view.\"\n",
    "\n",
    "Date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures\n",
    "\n",
    "Files Included:\n",
    "- Global Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv)\n",
    "- Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv)\n",
    "\n",
    "\n",
    "The raw data comes from the [Berkeley Earth data page](http://berkeleyearth.org/data/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data from Kaggle API\n",
    "# You must have an account with Kaggle\n",
    "kaggle_download('berkeleyearth/climate-change-earth-surface-temperature-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "directory = os.path.realpath(\"..\") + '/global-land-temp-dw/'\n",
    "tables = ['GlobalLandTemperaturesByMajorCity.csv', 'GlobalLandTemperaturesByCity.csv']\n",
    "\n",
    "df_major_city = pd.read_csv(directory + \"data/\" + tables[0])\n",
    "df_city = pd.read_csv(directory + \"data/\" + tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DF for info major cities\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 239177 entries, 0 to 239176\nData columns (total 7 columns):\n #   Column                         Non-Null Count   Dtype  \n---  ------                         --------------   -----  \n 0   dt                             239177 non-null  object \n 1   AverageTemperature             228175 non-null  float64\n 2   AverageTemperatureUncertainty  228175 non-null  float64\n 3   City                           239177 non-null  object \n 4   Country                        239177 non-null  object \n 5   Latitude                       239177 non-null  object \n 6   Longitude                      239177 non-null  object \ndtypes: float64(2), object(5)\nmemory usage: 12.8+ MB\n\nDF info for cities\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8599212 entries, 0 to 8599211\nData columns (total 7 columns):\n #   Column                         Dtype  \n---  ------                         -----  \n 0   dt                             object \n 1   AverageTemperature             float64\n 2   AverageTemperatureUncertainty  float64\n 3   City                           object \n 4   Country                        object \n 5   Latitude                       object \n 6   Longitude                      object \ndtypes: float64(2), object(5)\nmemory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"DF for info major cities\")\n",
    "df_major_city.info()\n",
    "\n",
    "print(\"\\nDF info for cities\")\n",
    "df_city.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null/Missing Values for major cities\n",
      "dt                                   0\n",
      "AverageTemperature               11002\n",
      "AverageTemperatureUncertainty    11002\n",
      "City                                 0\n",
      "Country                              0\n",
      "Latitude                             0\n",
      "Longitude                            0\n",
      "dtype: int64\n",
      "\n",
      "Null/Missing Values for cities\n",
      "dt                                    0\n",
      "AverageTemperature               364130\n",
      "AverageTemperatureUncertainty    364130\n",
      "City                                  0\n",
      "Country                               0\n",
      "Latitude                              0\n",
      "Longitude                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Null/Missing Values\n",
    "df_city.isnull().sum()\n",
    "\n",
    "print(\"Null/Missing Values for major cities\")\n",
    "print(df_major_city.isnull().sum())\n",
    "\n",
    "print(\"\\nNull/Missing Values for cities\")\n",
    "print(df_city.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of rows before removing NA: 239,177\n",
      "Number of rows after removing NA: 228,175\n",
      "Number of rows before removing NA: 8,599,212\n",
      "Number of rows after removing NA: 8,235,082\n"
     ]
    }
   ],
   "source": [
    "# Drop NA/missing values This data requires all values to be available\n",
    "def drop_na(pd_df):\n",
    "\n",
    "    print(f'Number of rows before removing NA: {pd_df.shape[0]:,}')\n",
    "\n",
    "    pd_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    print(f'Number of rows after removing NA: {pd_df.shape[0]:,}')\n",
    "\n",
    "\n",
    "drop_na(df_major_city)\n",
    "drop_na(df_city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for Boolean Major city\n",
    "df_major_city = df_major_city.assign(major_city='true')\n",
    "df_city = df_city.assign(major_city='false')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both datasets\n",
    "dframes = [df_major_city, df_city]\n",
    "df = pd.concat(dframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "0  1849-01-01              26.704                          1.435  Abidjan   \n",
       "1  1849-02-01              27.434                          1.362  Abidjan   \n",
       "2  1849-03-01              28.101                          1.612  Abidjan   \n",
       "3  1849-04-01              26.140                          1.387  Abidjan   \n",
       "4  1849-05-01              25.427                          1.200  Abidjan   \n",
       "\n",
       "         Country Latitude Longitude major_city  \n",
       "0  Côte D'Ivoire    5.63N     3.23W       true  \n",
       "1  Côte D'Ivoire    5.63N     3.23W       true  \n",
       "2  Côte D'Ivoire    5.63N     3.23W       true  \n",
       "3  Côte D'Ivoire    5.63N     3.23W       true  \n",
       "4  Côte D'Ivoire    5.63N     3.23W       true  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dt</th>\n      <th>AverageTemperature</th>\n      <th>AverageTemperatureUncertainty</th>\n      <th>City</th>\n      <th>Country</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>major_city</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1849-01-01</td>\n      <td>26.704</td>\n      <td>1.435</td>\n      <td>Abidjan</td>\n      <td>Côte D'Ivoire</td>\n      <td>5.63N</td>\n      <td>3.23W</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1849-02-01</td>\n      <td>27.434</td>\n      <td>1.362</td>\n      <td>Abidjan</td>\n      <td>Côte D'Ivoire</td>\n      <td>5.63N</td>\n      <td>3.23W</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1849-03-01</td>\n      <td>28.101</td>\n      <td>1.612</td>\n      <td>Abidjan</td>\n      <td>Côte D'Ivoire</td>\n      <td>5.63N</td>\n      <td>3.23W</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1849-04-01</td>\n      <td>26.140</td>\n      <td>1.387</td>\n      <td>Abidjan</td>\n      <td>Côte D'Ivoire</td>\n      <td>5.63N</td>\n      <td>3.23W</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1849-05-01</td>\n      <td>25.427</td>\n      <td>1.200</td>\n      <td>Abidjan</td>\n      <td>Côte D'Ivoire</td>\n      <td>5.63N</td>\n      <td>3.23W</td>\n      <td>true</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by year and reset index\n",
    "df.sort_values(by='dt',inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "0    1743-11-01               7.067                          1.839   \n",
       "1    1743-11-01               2.840                          1.887   \n",
       "2    1743-11-01               7.807                          1.816   \n",
       "3    1743-11-01               6.176                          2.264   \n",
       "4    1743-11-01               7.541                          1.753   \n",
       "..          ...                 ...                            ...   \n",
       "995  1744-04-01               8.810                          2.688   \n",
       "996  1744-04-01              12.943                          2.166   \n",
       "997  1744-04-01              11.596                          2.044   \n",
       "998  1744-04-01               8.296                          2.501   \n",
       "999  1744-04-01               9.182                          2.511   \n",
       "\n",
       "                City         Country Latitude Longitude major_city  \n",
       "0      West Bromwich  United Kingdom   52.24N     2.63W      false  \n",
       "1               Lvov         Ukraine   50.63N    24.08E      false  \n",
       "2             Venice           Italy   45.81N    12.69E      false  \n",
       "3       Stara Zagora        Bulgaria   42.59N    26.18E      false  \n",
       "4              Luton  United Kingdom   52.24N     0.00W      false  \n",
       "..               ...             ...      ...       ...        ...  \n",
       "995            Mazyr         Belarus   52.24N    28.91E      false  \n",
       "996           Edirne          Turkey   42.59N    26.18E      false  \n",
       "997  Aix En Provence          France   44.20N     4.47E      false  \n",
       "998            Luton  United Kingdom   52.24N     0.00W      false  \n",
       "999     Staryy Oskol          Russia   50.63N    36.76E      false  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dt</th>\n      <th>AverageTemperature</th>\n      <th>AverageTemperatureUncertainty</th>\n      <th>City</th>\n      <th>Country</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>major_city</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1743-11-01</td>\n      <td>7.067</td>\n      <td>1.839</td>\n      <td>West Bromwich</td>\n      <td>United Kingdom</td>\n      <td>52.24N</td>\n      <td>2.63W</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1743-11-01</td>\n      <td>2.840</td>\n      <td>1.887</td>\n      <td>Lvov</td>\n      <td>Ukraine</td>\n      <td>50.63N</td>\n      <td>24.08E</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1743-11-01</td>\n      <td>7.807</td>\n      <td>1.816</td>\n      <td>Venice</td>\n      <td>Italy</td>\n      <td>45.81N</td>\n      <td>12.69E</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1743-11-01</td>\n      <td>6.176</td>\n      <td>2.264</td>\n      <td>Stara Zagora</td>\n      <td>Bulgaria</td>\n      <td>42.59N</td>\n      <td>26.18E</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1743-11-01</td>\n      <td>7.541</td>\n      <td>1.753</td>\n      <td>Luton</td>\n      <td>United Kingdom</td>\n      <td>52.24N</td>\n      <td>0.00W</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>1744-04-01</td>\n      <td>8.810</td>\n      <td>2.688</td>\n      <td>Mazyr</td>\n      <td>Belarus</td>\n      <td>52.24N</td>\n      <td>28.91E</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>1744-04-01</td>\n      <td>12.943</td>\n      <td>2.166</td>\n      <td>Edirne</td>\n      <td>Turkey</td>\n      <td>42.59N</td>\n      <td>26.18E</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>1744-04-01</td>\n      <td>11.596</td>\n      <td>2.044</td>\n      <td>Aix En Provence</td>\n      <td>France</td>\n      <td>44.20N</td>\n      <td>4.47E</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>1744-04-01</td>\n      <td>8.296</td>\n      <td>2.501</td>\n      <td>Luton</td>\n      <td>United Kingdom</td>\n      <td>52.24N</td>\n      <td>0.00W</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>1744-04-01</td>\n      <td>9.182</td>\n      <td>2.511</td>\n      <td>Staryy Oskol</td>\n      <td>Russia</td>\n      <td>50.63N</td>\n      <td>36.76E</td>\n      <td>false</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dt                                object\n",
       "AverageTemperature               float64\n",
       "AverageTemperatureUncertainty    float64\n",
       "City                              object\n",
       "Country                           object\n",
       "Latitude                          object\n",
       "Longitude                         object\n",
       "major_city                        object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Confirm Data types are as expected\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV to load into SQL redshift\n",
    "df.to_csv(directory + \"data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Get Bucket Name from configurations & upload file to S3\n",
    "config = ConfigParser()\n",
    "config.read('DW.cfg')\n",
    "file_log = config.get('S3', 'log_data')\n",
    "\n",
    "upload_file(directory + \"data/dataset.csv\", file_log, \"logs/log_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "For this project, a Star Schema was selected because of it simple style and its effectiveness to handle simple queries.\n",
    "\n",
    "![Data Model](media/data_model.png 'Data Model')\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Necessary steps to pipeline the data into the chosen data model\n",
    "\n",
    "- A Kaggle API is used to get and download the data\n",
    "- Pandas is used to explore and clean the data\n",
    "- Dataframe is saved as CSV file and uploaded to AWS s3 bucket\n",
    "- Data is imported to AWS Redshift\n",
    "- Data quality tests are carried out to make sure the data exists and is available "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "source": [
    "# Get Database Variables\n",
    "config = ConfigParser()\n",
    "config.read('DW.cfg')\n",
    "\n",
    "print(\"{}, {}, {}, {}, {}\".format(*config['CLUSTER'].values()))\n",
    "\n",
    "\n",
    "# Connect to redshift database\n",
    "conn = psycopg2.connect(\"\"\"host={} \n",
    "                           dbname={} \n",
    "                           user={} \n",
    "                           password={}\n",
    "                           port={}\"\"\"\\\n",
    "                           .format(*config['CLUSTER'].values()    \n",
    "))\n",
    "\n",
    "# conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dwhcluster.cedlo7g6palf.eu-west-1.redshift.amazonaws.com, landtempdb, dwhuser, Passw0rd, 5439\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all table if exists\n",
    "for query in sql_drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "for query in sql_create_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to Staging\n",
    "s3_bucket = \"s3://{}/logs/\".format(config.get('S3','log_data'))\n",
    "\n",
    "sql_query = staging_insert.format(\n",
    "    s3_bucket,\n",
    "    config.get('IAM_ROLE','ARN'),\n",
    "    config.get('CLUSTER','dwh_region')\n",
    "    )\n",
    "# print(sql_query)\n",
    "cur.execute(sql_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data from staging to facts and dimensions\n",
    "for query in sql_insert_tables:\n",
    "    # print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "The Data pipeline has the following data Inegrity Constraints:\n",
    "- Each Column have a Data Type constraints\n",
    "- Duplicates are avoided by using runing a subquery to check if that value is not already in the table i.e. `WHERE dt NOT IN (SELECT DISTINCT dt FROM time)`\n",
    "\n",
    "Data in the Schema is evaluated that table exists and it has rows\n",
    "Data Types for all columns are tested to confirm they are as expected\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality check Data data types:\n",
    "reading_by_city_data_types = [\n",
    "    'int',\n",
    "    'datetime.date',\n",
    "    'decimal.Decimal',\n",
    "    'decimal.Decimal',\n",
    "    'decimal.Decimal',\n",
    "    'bool'\n",
    "]\n",
    "\n",
    "cities_data_types = [\n",
    "    'int',\n",
    "    'str',\n",
    "    'str',\n",
    "    'str',\n",
    "    'str',\n",
    "    'bool'\n",
    "]\n",
    "\n",
    "dt_data_types = [\n",
    "    'datetime.date',\n",
    "    'int',\n",
    "    'int',\n",
    "    'int',\n",
    "    'int',\n",
    "    'int'\n",
    "]\n",
    "\n",
    "# Tables Data Types List \n",
    "tables_data_types = [\n",
    "    reading_by_city_data_types,\n",
    "    cities_data_types, \n",
    "    dt_data_types\n",
    "]\n",
    "\n",
    "# Tables to Check\n",
    "tables = ['readings_by_city', 'cities', 'time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Quality SUCCESS for table: readings_by_city. Total rows: 8609236\nData Quality SUCCESS for table: cities. Total rows: 3610\nData Quality SUCCESS for table: time. Total rows: 3167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for table in tables:\n",
    "\n",
    "    data_quality_check(conn, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Integrity SUCCESS for table: readings_by_city\nData Integrity SUCCESS for table: cities\nData Integrity SUCCESS for table: time\n"
     ]
    }
   ],
   "source": [
    "# Data Integrity Check for Data Types\n",
    "\n",
    "for table, data_types in zip(tables, tables_data_types):\n",
    "\n",
    "    data_integrity_check(conn, table, data_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "![Data Dictionary](media/data_dict.JPG 'Data Dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = [\n",
    "    {   \n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'city_id',\n",
    "        'data type': 'BIGINT IDENTITY(0,1)',\n",
    "        'data format': 'NNNNNNN',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'Primary Key Index number for each row',\n",
    "        'example':'03'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'city',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': '',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'city name, comes from staging_events',\n",
    "        'example':'Barcelona'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'country',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': '',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'country name, comes from staging_events',\n",
    "        'example':'Venezuela'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'latitude',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': 'NN.NT',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'city latitude, comes from staging_events',\n",
    "        'example':'12.5N'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'longitude',\n",
    "        'data type': 'TEXT',\n",
    "        'data format': 'NN.NT',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'city longitude, comes from staging_events',\n",
    "        'example':'12.5E'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension cities',\n",
    "        'field name': 'major_city',\n",
    "        'data type': 'BOOLEAN',\n",
    "        'data format': 'true/false',\n",
    "        'field size': '5',\n",
    "        'description':'Boolean that identifies if city is major or not from staging_events',\n",
    "        'example':'true'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'dt',\n",
    "        'data type': 'DATE',\n",
    "        'data format': 'YYYY-MM-DD',\n",
    "        'field size': '10',\n",
    "        'description':'date of temperature measure, comes from staging_events',\n",
    "        'example':'2018-02-07'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'day',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'day of temperature measure, comes from column dt',\n",
    "        'example':'07'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'month',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'month of temperature measure, comes from column dt',\n",
    "        'example':'02'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'week',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'week of temperature measure, comes from column dt',\n",
    "        'example':'02'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'weekday',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NN',\n",
    "        'field size': '2',\n",
    "        'description':'week day number of temperature measure, comes from column dt',\n",
    "        'example':'02'\n",
    "    },\n",
    "    {\n",
    "        'table':'Dimension time',\n",
    "        'field name': 'year',\n",
    "        'data type': 'SMALLINT',\n",
    "        'data format': 'NNNN',\n",
    "        'field size': '4',\n",
    "        'description':'year of temperature measure, comes from column dt',\n",
    "        'example':'2018'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'by_city_id',\n",
    "        'data type': 'BIGINT IDENTITY(0,1)',\n",
    "        'data format': 'NNNNNN',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'Primary Key Index number for each row',\n",
    "        'example':'2'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'city_id',\n",
    "        'data type': 'NUMERIC',\n",
    "        'data format': 'NNNNNN',\n",
    "        'field size': 'VARIABLE',\n",
    "        'description':'Foreign Key Index number from cities table',\n",
    "        'example':'4525'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'avg_temp',\n",
    "        'data type': 'NUMERIC(7,3)',\n",
    "        'data format': 'NNNN.NNN',\n",
    "        'field size': '7',\n",
    "        'description':'temperature reading comes from staging_events',\n",
    "        'example':'25.025'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'avg_temp_uncertainty',\n",
    "        'data type': 'NUMERIC(7,3)',\n",
    "        'data format': 'NNNN.NNN',\n",
    "        'field size': '7',\n",
    "        'description':'temperature uncertainty reading comes from staging_events',\n",
    "        'example':'1.025'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'date',\n",
    "        'data type': 'DATE',\n",
    "        'data format': 'YYYY-MM-DD',\n",
    "        'field size': '7',\n",
    "        'description':'date of temperature measure, comes from staging_events',\n",
    "        'example':'2018-02-07'\n",
    "    },\n",
    "    {\n",
    "        'table':'Fact readings_by_city',\n",
    "        'field name': 'major_city',\n",
    "        'data type': 'BOLEAN',\n",
    "        'data format': '',\n",
    "        'field size': '5',\n",
    "        'description':'Boolean that identifies if city is major or not from staging_events',\n",
    "        'example':'true'\n",
    "    },\n",
    "    \n",
    "    \n",
    "       \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    table            field name             data type  \\\n",
       "0        Dimension cities               city_id  BIGINT IDENTITY(0,1)   \n",
       "1        Dimension cities                  city                  TEXT   \n",
       "2        Dimension cities               country                  TEXT   \n",
       "3        Dimension cities              latitude                  TEXT   \n",
       "4        Dimension cities             longitude                  TEXT   \n",
       "5        Dimension cities            major_city               BOOLEAN   \n",
       "6          Dimension time                    dt                  DATE   \n",
       "7          Dimension time                   day              SMALLINT   \n",
       "8          Dimension time                 month              SMALLINT   \n",
       "9          Dimension time                  week              SMALLINT   \n",
       "10         Dimension time               weekday              SMALLINT   \n",
       "11         Dimension time                  year              SMALLINT   \n",
       "12  Fact readings_by_city            by_city_id  BIGINT IDENTITY(0,1)   \n",
       "13  Fact readings_by_city               city_id               NUMERIC   \n",
       "14  Fact readings_by_city              avg_temp          NUMERIC(7,3)   \n",
       "15  Fact readings_by_city  avg_temp_uncertainty          NUMERIC(7,3)   \n",
       "16  Fact readings_by_city                  date                  DATE   \n",
       "17  Fact readings_by_city            major_city                BOLEAN   \n",
       "\n",
       "   data format field size                                        description  \\\n",
       "0      NNNNNNN   VARIABLE              Primary Key Index number for each row   \n",
       "1                VARIABLE               city name, comes from staging_events   \n",
       "2                VARIABLE            country name, comes from staging_events   \n",
       "3        NN.NT   VARIABLE           city latitude, comes from staging_events   \n",
       "4        NN.NT   VARIABLE          city longitude, comes from staging_events   \n",
       "5   true/false          5  Boolean that identifies if city is major or no...   \n",
       "6   YYYY-MM-DD         10  date of temperature measure, comes from stagin...   \n",
       "7           NN          2   day of temperature measure, comes from column dt   \n",
       "8           NN          2  month of temperature measure, comes from colum...   \n",
       "9           NN          2  week of temperature measure, comes from column dt   \n",
       "10          NN          2  week day number of temperature measure, comes ...   \n",
       "11        NNNN          4  year of temperature measure, comes from column dt   \n",
       "12      NNNNNN   VARIABLE              Primary Key Index number for each row   \n",
       "13      NNNNNN   VARIABLE         Foreign Key Index number from cities table   \n",
       "14    NNNN.NNN          7      temperature reading comes from staging_events   \n",
       "15    NNNN.NNN          7  temperature uncertainty reading comes from sta...   \n",
       "16  YYYY-MM-DD          7  date of temperature measure, comes from stagin...   \n",
       "17                      5  Boolean that identifies if city is major or no...   \n",
       "\n",
       "       example  \n",
       "0           03  \n",
       "1    Barcelona  \n",
       "2    Venezuela  \n",
       "3        12.5N  \n",
       "4        12.5E  \n",
       "5         true  \n",
       "6   2018-02-07  \n",
       "7           07  \n",
       "8           02  \n",
       "9           02  \n",
       "10          02  \n",
       "11        2018  \n",
       "12           2  \n",
       "13        4525  \n",
       "14      25.025  \n",
       "15       1.025  \n",
       "16  2018-02-07  \n",
       "17        true  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>table</th>\n      <th>field name</th>\n      <th>data type</th>\n      <th>data format</th>\n      <th>field size</th>\n      <th>description</th>\n      <th>example</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dimension cities</td>\n      <td>city_id</td>\n      <td>BIGINT IDENTITY(0,1)</td>\n      <td>NNNNNNN</td>\n      <td>VARIABLE</td>\n      <td>Primary Key Index number for each row</td>\n      <td>03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dimension cities</td>\n      <td>city</td>\n      <td>TEXT</td>\n      <td></td>\n      <td>VARIABLE</td>\n      <td>city name, comes from staging_events</td>\n      <td>Barcelona</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dimension cities</td>\n      <td>country</td>\n      <td>TEXT</td>\n      <td></td>\n      <td>VARIABLE</td>\n      <td>country name, comes from staging_events</td>\n      <td>Venezuela</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dimension cities</td>\n      <td>latitude</td>\n      <td>TEXT</td>\n      <td>NN.NT</td>\n      <td>VARIABLE</td>\n      <td>city latitude, comes from staging_events</td>\n      <td>12.5N</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dimension cities</td>\n      <td>longitude</td>\n      <td>TEXT</td>\n      <td>NN.NT</td>\n      <td>VARIABLE</td>\n      <td>city longitude, comes from staging_events</td>\n      <td>12.5E</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Dimension cities</td>\n      <td>major_city</td>\n      <td>BOOLEAN</td>\n      <td>true/false</td>\n      <td>5</td>\n      <td>Boolean that identifies if city is major or no...</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Dimension time</td>\n      <td>dt</td>\n      <td>DATE</td>\n      <td>YYYY-MM-DD</td>\n      <td>10</td>\n      <td>date of temperature measure, comes from stagin...</td>\n      <td>2018-02-07</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Dimension time</td>\n      <td>day</td>\n      <td>SMALLINT</td>\n      <td>NN</td>\n      <td>2</td>\n      <td>day of temperature measure, comes from column dt</td>\n      <td>07</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Dimension time</td>\n      <td>month</td>\n      <td>SMALLINT</td>\n      <td>NN</td>\n      <td>2</td>\n      <td>month of temperature measure, comes from colum...</td>\n      <td>02</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Dimension time</td>\n      <td>week</td>\n      <td>SMALLINT</td>\n      <td>NN</td>\n      <td>2</td>\n      <td>week of temperature measure, comes from column dt</td>\n      <td>02</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Dimension time</td>\n      <td>weekday</td>\n      <td>SMALLINT</td>\n      <td>NN</td>\n      <td>2</td>\n      <td>week day number of temperature measure, comes ...</td>\n      <td>02</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Dimension time</td>\n      <td>year</td>\n      <td>SMALLINT</td>\n      <td>NNNN</td>\n      <td>4</td>\n      <td>year of temperature measure, comes from column dt</td>\n      <td>2018</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Fact readings_by_city</td>\n      <td>by_city_id</td>\n      <td>BIGINT IDENTITY(0,1)</td>\n      <td>NNNNNN</td>\n      <td>VARIABLE</td>\n      <td>Primary Key Index number for each row</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Fact readings_by_city</td>\n      <td>city_id</td>\n      <td>NUMERIC</td>\n      <td>NNNNNN</td>\n      <td>VARIABLE</td>\n      <td>Foreign Key Index number from cities table</td>\n      <td>4525</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Fact readings_by_city</td>\n      <td>avg_temp</td>\n      <td>NUMERIC(7,3)</td>\n      <td>NNNN.NNN</td>\n      <td>7</td>\n      <td>temperature reading comes from staging_events</td>\n      <td>25.025</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Fact readings_by_city</td>\n      <td>avg_temp_uncertainty</td>\n      <td>NUMERIC(7,3)</td>\n      <td>NNNN.NNN</td>\n      <td>7</td>\n      <td>temperature uncertainty reading comes from sta...</td>\n      <td>1.025</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Fact readings_by_city</td>\n      <td>date</td>\n      <td>DATE</td>\n      <td>YYYY-MM-DD</td>\n      <td>7</td>\n      <td>date of temperature measure, comes from stagin...</td>\n      <td>2018-02-07</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Fact readings_by_city</td>\n      <td>major_city</td>\n      <td>BOLEAN</td>\n      <td></td>\n      <td>5</td>\n      <td>Boolean that identifies if city is major or no...</td>\n      <td>true</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "data_dic_df = pd.DataFrame(data_dictionary)\n",
    "\n",
    "data_dic_df.to_csv(directory + \"media/data_dict.csv\")\n",
    "\n",
    "data_dic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "- Kaggle API: This allows the pipeline to run completely on scripts.\n",
    "\n",
    "- AWS Redshift & PostgreSQL: Redshift makes it easy to import data from s3 and enables you to mantain fast query performance. \n",
    "\n",
    "- AWS s3: Provides a platform for storage of data in the form of objects. \n",
    "\n",
    "- Anaconda3: Use for managing Python dependencies and working virtual environment\n",
    "\n",
    "\n",
    "Propose how often the data should be updated and why.\n",
    "- As per documentation from the Climate Change: Earth Surface Temperature Data Set, this can be updated on a monthly basis. as this is how often the source is updated.\n",
    "\n",
    "Write a description of how you would approach the problem differently under the following scenarios:\n",
    "- The data was increased by 100x.\n",
    "\n",
    "    - In case of an increased of size by magnitude of 100x, using pandas to manipulate the data would not pre practical, in this instance Spark would be the better choice and data should be partitioned\n",
    "\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "    - For this specific data set, the data will not change daily, but assiming this scenario, once the data is available in the DW, it can be accessed (i.e. using a python script to obtain the data and create the dashboar.) Assuming daily updates a scheduler tool can be use like Apache Airflow.\n",
    "\n",
    "- The database needed to be accessed by 100+ people.\n",
    "\n",
    "    - Depending on the settings used to create the AWS Redshift cluster, the Data Warehouse can be configured to allow access to as much as users as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('land-temp-dw': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6928e04d17b1ec80b230eec5c511cba50c562c46b302dc929486dd2dac5f22f5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}