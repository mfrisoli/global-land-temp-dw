{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "from helpers.sql import sql_create_tables, staging_insert, sql_drop_tables\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "directory = os.path.realpath(\"..\") + '/global-land-temp-dw/data/'\n",
    "tables = ['GlobalLandTemperaturesByMajorCity.csv', 'GlobalLandTemperaturesByCity.csv']\n",
    "\n",
    "df_major_city = pd.read_csv(directory + tables[0])\n",
    "df_city = pd.read_csv(directory + tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values This data requires all values to be available\n",
    "\n",
    "def drop_na(pd_df):\n",
    "\n",
    "    print(f'Number of rows before removing NA: {pd_df.shape[0]:,}')\n",
    "\n",
    "    pd_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    print(f'Number of rows after removing NA: {pd_df.shape[0]:,}')\n",
    "\n",
    "\n",
    "drop_na(df_major_city)\n",
    "drop_na(df_city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for Boolean Major city\n",
    "df_major_city = df_major_city.assign(major_city='true')\n",
    "df_city = df_city.assign(major_city='false')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatnate both datasets\n",
    "dframes = [df_major_city, df_city]\n",
    "df = pd.concat(dframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by year\n",
    "df.sort_values(by='dt',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm Data types are as expected\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV to load into SQL redshift\n",
    "df.to_csv(directory + \"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to s3 Function from AWS Services https://boto3.amazonaws.com/v1/documentation/api/1.10.46/guide/s3-uploading-files.html\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Bucket Name from configurations \n",
    "config = ConfigParser()\n",
    "config.read('DW.cfg')\n",
    "file_log = config.get('S3', 'log_data')\n",
    "\n",
    "upload_file(directory + \"dataset.csv\", file_log, \"logs/log_data.csv\")\n",
    "\n",
    "#s3 = boto3.client('s3')\n",
    "#with open(directory + \"dataset.csv\", \"rb\") as f:\n",
    "#    s3.upload_fileobj(f, \"global-land-temp\", \"data_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "source": [
    "### Star Schema"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dwhcluster.cg959wyk8kyf.us-west-2.redshift.amazonaws.com, landtempdb, dwhuser, Passw0rd, 5439\n"
     ]
    }
   ],
   "source": [
    "# Get Database Variables\n",
    "config = ConfigParser()\n",
    "config.read('DW.cfg')\n",
    "\n",
    "print(\"{}, {}, {}, {}, {}\".format(*config['CLUSTER'].values()))\n",
    "\n",
    "\n",
    "# Connect to redshift database\n",
    "conn = psycopg2.connect(\"\"\"host={} \n",
    "                           dbname={} \n",
    "                           user={} \n",
    "                           password={}\n",
    "                           port={}\"\"\"\\\n",
    "                           .format(*config['CLUSTER'].values()    \n",
    "))\n",
    "\n",
    "# conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all table if exists\n",
    "for query in sql_drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "for query in sql_create_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n    COPY staging_events (\n        index,\n        dt,\n        AverageTemperature,\n        AverageTemperatureUncertainty,\n        City,\n        Country,\n        Latitude,\n        Longitude,\n        major_city\n    )\n    FROM  's3://global-land-temp/logs/'\n    CREDENTIALS 'aws_iam_role=arn:aws:iam::879294216748:role/dwhRole'\n    REGION 'eu-west-1'\n    DELIMITER ','\n    DATEFORMAT 'auto'\n    IGNOREHEADER 1\n    ;\n\n"
     ]
    }
   ],
   "source": [
    "# Copy data to Staging\n",
    "sql_query = staging_insert.format(\n",
    "    's3://global-land-temp/logs/',\n",
    "    config.get('IAM_ROLE','ARN'),\n",
    "    config.get('CLUSTER','dwh_region')\n",
    "    )\n",
    "print(sql_query)\n",
    "cur.execute(sql_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n    CREATE TABLE IF NOT EXISTS cities (\n        city_id BIGINT IDENTITY(0,1),\n        country_id NUMERIC,\n        city TEXT,\n        latitude TEXT,\n        longitude TEXT\n    );\n\n\n    CREATE TABLE IF NOT EXISTS time (\n        dt DATE,\n        year SMALLINT,\n        month SMALLINT,\n        day SMALLINT,\n        weekday SMALLINT,\n        week SMALLINT\n    );\n\n\n    CREATE TABLE IF NOT EXISTS readings_by_city (\n        by_city_id BIGINT IDENTITY(0,1),\n        time_id NUMERIC,\n        city_id NUMERIC,\n        avg_temp NUMERIC,\n        avf_temp_uncertainty NUMERIC,\n        major_city BOOLEAN\n    );\n\n\n    CREATE TABLE IF NOT EXISTS staging_events (\n        index NUMERIC,\n        dt DATE NOT NULL,\n        AverageTemperature NUMERIC,\n        AverageTemperatureUncertainty NUMERIC,\n        City TEXT,\n        Country TEXT,\n        Latitude TEXT,\n        Longitude TEXT,\n        major_city BOOLEAN\n    );\n\n"
     ]
    }
   ],
   "source": [
    "for query in sql_create_tables:\n",
    "    print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Connection to the DB\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}